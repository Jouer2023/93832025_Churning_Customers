# -*- coding: utf-8 -*-
"""Ass3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ab107iRnYHdOZ_NUKx9kGTPpZPUP_EiT
"""

'''Customer churn is a major problem and one of the most important concerns for large companies. Due to the direct effect on the revenues of the companies, especially in
 the telecom field, companies are seeking to develop means to predict potential customer churn. Therefore, finding factors that increase customer churn is important to take #
 necessary actions to reduce this churn. The main contribution of your work is to develop a churn prediction model that assists telecom operators in predicting customers who
  are most likely subject to churn. Perform the following operations as you create the much needed deep learning application.





Create a platform to host the model either web-based or desktop application

Allow users to use the application to enter new data and your model should predict if the supplied data of a new customer can result in a churn or not giving the confidence
 factor of the model

Record a short video to demonstrate how your application works
Create a README.md file to briefly describe your project, functionalities, etc. This should include a link to the video'''

#5 to 7 percent
#A low churn rate tends to mean that people are satisfied with a businessâ€™s services or have good job satisfaction. Industry professionals generally agree that an annual
#churn rate of 5 to 7 percent is a good range. Anything higher than those percentages may point to poor company performance

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

#Loading the dataset
df=pd.read_csv('/content/drive/My Drive/Copy of CustomerChurn_dataset.csv')

df.head()

#Using the given datasetLinks to an external site. extract the relevant features that can define a customer churn.
relevant_features = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'TechSupport',
                      'Contract', 'PaymentMethod', 'TotalCharges', 'Churn']

dff = df[relevant_features]

#Use your EDA(Exploratory Data Analysis) skills to find out which customer profiles relate to churning a lot.
import matplotlib.pyplot as plt

dff.head()

dff.describe()

dff.shape

#Try the correlation method

#Checking for suspected customer profiles that are likely to churn a lot
#First case is senior citizen men, who have a partner and dependents churn a lot
senior_men_partner_dependents = dff[(dff['SeniorCitizen'] == 1) & (dff['gender'] == 'Male') & (dff['Partner'] == 'Yes') & (dff['Dependents'] == 'Yes')] #Subset
senior_men_partner_dependents.head()

#Calculating the churn rate for the above
total_se_men_partner_dependents = len(senior_men_partner_dependents)
total_se_men_partner_dependents

#Calculating the number of churns
no_of_senior_churns = len(senior_men_partner_dependents[(senior_men_partner_dependents['Churn'] == 'No')])
no_of_senior_churns

#The churn rate
churn_rate = (no_of_senior_churns/total_se_men_partner_dependents) * 100
churn_rate
#Churn rate seems high

#Going to check a few more
#Single young breadwinners
single_dependents = dff[(dff['SeniorCitizen'] == 0) & (dff['Partner'] == 'No') & (dff['Dependents'] == 'Yes')] #Subset
single_dependents.head()

#Calculating the number of churns
no_of_single_churns = len(single_dependents[(single_dependents['Churn'] == 'No')])
no_of_single_churns

#Calculating the number of customers
total_single_dependents = len(single_dependents)
total_single_dependents

#The churn rate
churn_rate = (no_of_single_churns/total_single_dependents) * 100
churn_rate
#Churn rate seems high

dff.head()

#Checking for null values
null_values = dff.isnull() #Returns a boolean
null_values #False - does not contain a null value

null_values_sum = null_values.sum()
null_values_sum

#Encoding categorical variables
from sklearn.preprocessing import LabelEncoder
lab = LabelEncoder()

features = ['gender', 'Partner' , 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'TechSupport', 'Contract', 'PaymentMethod']

#Using a for loop to encode each column
for feature in features:
  df[feature] = lab.fit_transform(df[feature])


df['Churn'] = lab.fit_transform(df['Churn'])

dff.head()

#Checking correlation
correlation = df.corr()
correlation['Churn'].sort_values(ascending=False)

correlation
#Just checking the level of influence

#Using the features in (1) define and train a Multi-Layer Perceptron model using the Functional API
import numpy as np
import tensorflow as tf
import keras
from keras import layers
from sklearn.model_selection import train_test_split
#from tensorflow.keras.layers import Input, Dense, Dropout



dff.head()

df.head()



#Spliting dataset
dft = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'TechSupport',
                      'Contract', 'PaymentMethod', 'TotalCharges']
X = df[dft]
y = df['Churn']
Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.3,random_state=42)

#Input layer
inputs = keras.Input(shape=(len(dft),)) #This is a basic graph with three layers. To build this model using the functional API, start by creating an input node:

inputs.shape

inputs.dtype

#Hidden layers
dense = layers.Dense(64, activation="relu")
x = dense(inputs)

#Dropout layer
x = layers.Dropout(0.2)(x)

dense = layers.Dense(32, activation="relu")
x = dense(x)

#Dropout layer
x = layers.Dropout(0.2)(x)

#Output layer
outputs = layers.Dense(1, activation="sigmoid")(x)

#Creating the model
model = keras.Model(inputs=inputs, outputs=outputs, name="Churn_Model")

model.summary()

keras.utils.plot_model(model, "my_first_model.png")

#Compiling the model
model.compile(
    loss=keras.losses.binary_crossentropy,
    optimizer = 'Adam',
    metrics=['accuracy'],
)

#Classifier
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=300, random_state= 10)

#from sklearn.model_selection import cross_val_score
#all_accuracies = cross_val_score(classifier, Xtrain, Ytrain, cv=5) #Cross validation to compute the score five times
#all_accuracies

#all_accuracies.mean()

from sklearn.model_selection import GridSearchCV

#Hyperparameter grid for grid search
grid_param = {
    'n_estimators': [100, 300, 500, 800, 1000],
    'criterion': ['gini', 'entropy'],
    'bootstrap': [True, False]
}

'''#Performing the grid search
gd_sr = GridSearchCV(estimator=classifier,
                     param_grid=grid_param,
                     scoring='accuracy',
                     cv=5,
                     n_jobs=-1)

gd_sr.fit(Xtrain, Ytrain)'''

#best_parameters = gd_sr.best_params_
#best_parameters

#best_result = gd_sr.best_score_
#best_result

#Training the model
#history = model.fit(Xtrain, Ytrain, batch_size=32, epochs=10, validation_split=0.2)

